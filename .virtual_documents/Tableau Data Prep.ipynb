








import pandas as pd
import pylast
import time
from tqdm import tqdm  # For progress bars


# Load datasets
concerts = pd.read_csv("data/ConcertArchivesExportFeb22-25.csv")  # All concerts
twentyfour = pd.read_csv("data/2024concertsranked.csv")  # 2024 concerts with rankings
bestof24 = pd.read_csv("data/bestof24.csv")  # Best albums/songs of 2024
bestof23 = pd.read_csv("data/bestof23.csv")  # Best albums/songs of 2023
bestof22 = pd.read_csv("data/bestof22.csv")  # Best albums/songs of 2022
bestof21 = pd.read_csv("data/bestof21.csv")  # Best albums/songs of 2021
scrobbles = pd.read_csv("data/scrobbles.csv") #Every song I've listened to since May 2020





# Display the first few rows of each dataset
twentyfour.head()


twentyfour.shape


concerts.head()


concerts.shape


# Merge concerts and twentyfour
df_concerts = pd.merge(
    concerts,                      # Left dataset
    twentyfour,                    # Right dataset
    on=['Start Date', 'Concert Name', 'Bands', 'Venue', 'Location'],  # Key columns
    how='left'                     # Keep all rows from concerts, add matching rows from twentyfour
)

# Display the merged dataset
df_concerts.head()


df_concerts.shape


df_concerts['Location'].value_counts()


# Check for missing values in the merged dataset
print(df_concerts.isnull().sum())





# Drop End Date and Concert Name columns
df_concerts = df_concerts.drop(columns=['End Date', 'Concert Name'])
df_concerts





# Fill missing values
df_concerts['Ranking'] = df_concerts['Ranking'].fillna(0)
df_concerts['Rating'] = df_concerts['Rating'].fillna(0)
df_concerts['Paid'] = df_concerts['Paid'].fillna(1)  # Assume missing values mean paid
df_concerts['Discount'] = df_concerts['Discount'].fillna("Unknown")

# Display the updated df
df_concerts.head()


# Convert Start Date to datetime
df_concerts['Start Date'] = pd.to_datetime(df_concerts['Start Date'], format='%m/%d/%Y')

# Filter for 2024 data
df_2024 = df_concerts[df_concerts['Start Date'].dt.year == 2024]

# Display the filtered dataset
df_2024.head()





# Get unique venues
unique_venues = df_2024['Venue'].unique()

# Convert to a DataFrame
df_unique_venues = pd.DataFrame(unique_venues, columns=['Venue'])

# Display the unique venues
df_unique_venues


# Save to CSV
df_unique_venues.to_csv("data/unique_venues_2024.csv", index=False)





# Load the venue details CSV
venue_info = pd.read_csv("data/venue_info_2024.csv")

# Display the first few rows
venue_info.head()


## Merge Back with df_2024


# Merge the datasets
df_2024 = pd.merge(
    df_2024,                      # Left dataset
    venue_info,                   # Right dataset
    on='Venue',                   # Key column
    how='left'                    # Keep all rows from df_2024
)

# Display the updated dataset
df_2024.head()


# Define a mapping dictionary for venue names
venue_name_mapping = {
    'The Rave/Eagles Club': 'The Rave',
    'Marcus Performing Arts Center': 'Marcus Center for the Performing Arts',
    'Summerfest Grounds at Henry Maier Festival Park-M & I Bank Classic Rock Stage': 'Summerfest Grounds at Henry Maier Festival Park',
}

# Standardize venue names in df_2024
df_2024['Venue'] = df_2024['Venue'].replace(venue_name_mapping)

# Standardize venue names in venue_info
venue_info['Venue'] = venue_info['Venue'].replace(venue_name_mapping)



# Add a ranking column to each bestof dataset
bestof24['Rank'] = bestof24.index + 1  # Index starts at 0, so add 1
bestof23['Rank'] = bestof23.index + 1
bestof22['Rank'] = bestof22.index + 1
bestof21['Rank'] = bestof21.index + 1

# Display the updated bestof24 dataset
bestof24.head()


# Add a year column to each dataset
bestof24['Year'] = 2024
bestof23['Year'] = 2023
bestof22['Year'] = 2022
bestof21['Year'] = 2021

# Combine all bestof datasets
bestof_all = pd.concat([bestof24, bestof23, bestof22, bestof21], ignore_index=True)

# Display the combined dataset
bestof_all.head()


# Define a function to categorize albums
def categorize_album(rank):
    if rank <= 5:
        return 'Top 5'
    elif rank <= 10:
        return 'Top 10'
    elif rank <= 25:
        return 'Top 25'
    elif rank <= 50:
        return 'Top 50'
    elif rank <= 100:
        return 'Top 100'
    else:
        return 'Not in Top 100'

# Apply the function to create a new column
bestof_all['Tier'] = bestof_all['Rank'].apply(categorize_album)

# Display the updated dataset
bestof_all.head()


bestof_all.columns





# Drop unnecessary columns
bestof_all = bestof_all[['Artist Name(s)', 'Album Name', 'Release Date', 'Rank', 'Year', 'Tier']]

# Display the updated dataset
bestof_all.head()


# Define a function to extract the primary artist name
def extract_primary_artist(artist_name):
    if pd.isna(artist_name):
        return None
    # Handle special case for "Tyler, The Creator"
    if "Tyler, The Creator" in artist_name:
        return "Tyler, The Creator"
    # Split on comma and return the first artist
    return artist_name.split(',')[0].strip()

# Apply the function to create a new column
bestof_all['Primary Artist'] = bestof_all['Artist Name(s)'].apply(extract_primary_artist)

# Display the updated dataset
bestof_all.head()



bestof_all.columns





# Split the Bands column into multiple rows
df_exploded = df_2024.assign(Bands=df_2024['Bands'].str.split(' / ')).explode('Bands')

# Display the exploded dataset
df_exploded.head()


df_exploded.columns


from thefuzz import process
import re

# Ensure dates are datetime objects
scrobbles['Timestamp'] = pd.to_datetime(scrobbles['Timestamp'])
df_exploded['Start Date'] = pd.to_datetime(df_exploded['Start Date'])

# Function to clean artist names (removes featured artists, punctuation, etc.)
def clean_artist_name(artist):
    # Remove content in parentheses and after "feat." or "ft."
    artist = re.sub(r'\s*\(.*?\)|\s*feat\..*|ft\..*', '', artist, flags=re.IGNORECASE)
    return artist.strip()

# Clean scrobble artist names
scrobbles['Cleaned Artist'] = scrobbles['Artist'].apply(clean_artist_name)

# Fuzzy matching function to get closest artist match
def get_closest_artist(artist, artist_list, threshold=80):
    match, score = process.extractOne(artist, artist_list)
    return match if score >= threshold else None

# Count scrobbles for each artist before the concert date
def count_scrobbles_before_concert(row):
    concert_date = row['Start Date']
    concert_artist = row['Bands']
    
    # Get closest match for concert artist from cleaned artist list
    matched_artist = get_closest_artist(concert_artist, scrobbles['Cleaned Artist'].unique())
    
    if matched_artist:
        count = scrobbles[(scrobbles['Cleaned Artist'] == matched_artist) & 
                          (scrobbles['Timestamp'] < concert_date)].shape[0]
    else:
        count = 0
    
    return count

# Add the scrobble count to df_exploded
df_exploded['Scrobbles Before Concert'] = df_exploded.apply(count_scrobbles_before_concert, axis=1)

# Preview the updated dataframe
print(df_exploded[['Bands', 'Start Date', 'Scrobbles Before Concert']].head())



df_exploded.head()


df_exploded.columns


import pandas as pd
from datetime import datetime

# Convert release date in bestof_all to datetime
bestof_all['Release Date'] = pd.to_datetime(bestof_all['Release Date'])

# Merge the data on the artist and primary artist
merged_df = pd.merge(df_exploded, bestof_all, left_on='Bands', right_on='Primary Artist', how='left')

# Ensure 'Release Date' is not NaN before applying idxmax
most_recent_album = merged_df.dropna(subset=['Release Date']).groupby('Bands').apply(
    lambda x: x.loc[x['Release Date'].idxmax()] if not x['Release Date'].isnull().all() else pd.Series())
most_recent_album = most_recent_album[['Bands', 'Album Name', 'Release Date', 'Tier']]

# Reset index to avoid ambiguity in merge
merged_df = merged_df.reset_index(drop=True)
most_recent_album = most_recent_album.reset_index(drop=True)

# Merge this back into the original exploded dataframe
merged_df = pd.merge(df_exploded, most_recent_album, left_on='Bands', right_on='Bands', how='left')

# Calculate wait time for each concert
def calculate_wait_time(row):
    if pd.isna(row['Release Date']):
        return 'N/A'
    concert_date = pd.to_datetime(row['Start Date'])
    release_date = row['Release Date']
    delta = concert_date - release_date
    years = delta.days // 365
    months = (delta.days % 365) // 30
    days = (delta.days % 365) % 30
    if years == 0 and months == 0 and days == 0:
        return f"{years} years, {months} months, {days} days"
    return f"{years} years, {months} months, {days} days"

# Apply the wait time calculation
merged_df['Wait Time'] = merged_df.apply(calculate_wait_time, axis=1)

# Format the album and tier
merged_df['Album Info'] = merged_df.apply(
    lambda row: f"{row['Tier']} ({row['Release Date'].year}) - {row['Album Name']}" if pd.notna(row['Album Name']) else "N/A", 
    axis=1)

# Merge df_exploded with bestof_all
merged_df = pd.merge(df_exploded, bestof_all, left_on='Bands', right_on='Primary Artist', how='left')

# Drop rows with NaN in the 'Release Date' column
merged_df_cleaned = merged_df.dropna(subset=['Release Date'])

# Ensure 'Release Date' is not NaN before applying idxmax
most_recent_album = merged_df_cleaned.dropna(subset=['Release Date']).groupby('Bands').apply(
    lambda x: x.loc[x['Release Date'].idxmax()] if not x['Release Date'].isnull().all() else pd.Series())
most_recent_album = most_recent_album[['Bands', 'Album Name', 'Release Date', 'Tier']]

# Reset index again to avoid ambiguity in merge
merged_df_cleaned = merged_df_cleaned.reset_index(drop=True)
most_recent_album = most_recent_album.reset_index(drop=True)

# Merge this back into the original exploded dataframe
merged_df = pd.merge(df_exploded, most_recent_album, left_on='Bands', right_on='Bands', how='left')

# Display the final dataset
merged_df



merged_df.columns


merged_df.to_excel('data/cleaned24.xlsx', index=False)





# Load the Excel file into a DataFrame
final_data = pd.read_excel('data/cleaned24withlocalindicator.xlsx')

# View the first few rows to ensure it's loaded correctly
final_data.head()


final_data.columns


# Ensure date columns are in datetime format
final_data['Start Date'] = pd.to_datetime(final_data['Start Date'], errors='coerce')
final_data['Release Date'] = pd.to_datetime(final_data['Release Date'], errors='coerce')

# Calculate the 'Waited' column
final_data['Waited'] = final_data.apply(
    lambda row: f"{(row['Start Date'] - row['Release Date']).days} day(s)" 
    if pd.notnull(row['Release Date']) and row['Start Date'] >= row['Release Date'] else '',
    axis=1
)

# Save the updated DataFrame to a new Excel file
final_data.to_excel('final_data_with_waited_column.xlsx', index=False)

# Check the output
final_data[['Bands', 'Start Date', 'Release Date', 'Waited']]


final_data


final_data.dtypes


from geopy.geocoders import Nominatim
import pandas as pd

# Initialize the geolocator
geolocator = Nominatim(user_agent="concert_geocoder")

# Function to get the full address from the location
def get_address(location):
    try:
        # Geocode the location
        location_info = geolocator.geocode(location, timeout=10)
        if location_info:
            return location_info.address  # Full address
        else:
            return None
    except Exception as e:
        print(f"Error geocoding {location}: {e}")
        return None

# Create a 'Venue Location' column to identify unique venue-location pairs
final_data['Venue Location'] = final_data['Venue'] + ', ' + final_data['Location']

# Identify unique venue-location pairs
unique_locations = final_data['Venue Location'].drop_duplicates()

# Geocode unique locations
location_addresses = unique_locations.apply(get_address)

# Create a mapping from 'Venue Location' to 'Full Address'
location_to_address = dict(zip(unique_locations, location_addresses))

# Map the 'Full Address' back to the original dataset
final_data['Full Address'] = final_data['Venue Location'].map(location_to_address)



# Filter rows where 'Full Address' is NaN
venues_without_address = final_data[final_data['Full Address'].isna()]

# Display the venues without addresses
venues_without_address[['Venue', 'Location']]



# Dictionary mapping venues to their addresses
venue_addresses = {
    'Pool Studios': '3728 N Fratney St, Milwaukee, WI 53212',
    'Linneman\'s Riverwest Inn': '1001 E Locust St, Milwaukee, WI 53212',
    'Paradigm Coffee and Music': '1202 N 8th St, Sheboygan, WI 53081',
    'Lilliput Records': '1669 N Farwell Ave, Milwaukee, WI 53202',
    'Summerfest Grounds at Henry Maier Festival Park': '200 N Harbor Dr, Milwaukee, WI 53202',
    'Peck Pavilion, Marcus Center for the Performing Arts': '929 N Water St, Milwaukee, WI 53202'
}

# Update the 'Full Address' column based on the 'Venue' column
final_data['Full Address'] = final_data['Venue'].replace(venue_addresses)



final_data.isna().sum()


final_data[final_data['Is Local'].isna()]


# Update the 'Is Local' value for the specific concert
final_data.at[36, 'Is Local'] = 0  # 0 indicates the artist is not local


final_data.isna().sum()


final_data.to_csv('data/final_data.csv', index=False)


import pandas as pd

# Load your data
df = pd.read_csv('final_data.csv')

# Group by unique concert identifier (Start Date + Venue)
aggregated_data = df.groupby(['Start Date', 'Venue', 'Location', 'capacity']).agg({
    'Bands': lambda x: ', '.join(x),  # Combine bands into a single string
    'Is Local': 'max',                # 1 if any band is local
    'Ranking': 'min',                 # Best ranking
    'Rating': 'max',                  # Highest rating
    'Paid': 'max',                    # 1 if any ticket was paid
    'Discount': 'first',              # First discount reason
    'dist_from_home': 'first',        # First distance
    'drive_time': 'first',            # First drive time
    'Scrobbles Before Concert': 'sum',  # Sum scrobbles across all bands
    'Album Name': 'first',            # First album name
    'Release Date': 'first',          # First release date
    'Tier': 'first',                  # First tier
    'Waited': 'first',                # First wait time
    'Venue Location': 'first',        # First venue location
    'Full Address': 'first',          # First full address
    'Latitude': 'first',              # First latitude
    'Longitude': 'first'              # First longitude
}).reset_index()

# Save the aggregated data to a new CSV
aggregated_data.to_csv('final_data_aggregated.csv', index=False)


aggregated_data.columns


import pandas as pd

# Load the already aggregated data
aggregated_data = pd.read_csv('final_data_aggregated.csv')

# Load the original dataset
df = pd.read_csv('final_data.csv')

# Keep the Marcus Center venues separate (no renaming)
df['Bands'] = df['Bands'].str.split(', ')

# Aggregate bands per concert (Start Date + Venue)
venue_bands = df.groupby(['Start Date', 'Venue'])['Bands'].sum().reset_index()

# Convert bands list back to a string
venue_bands['Bands'] = venue_bands['Bands'].apply(', '.join)

# Merge with the aggregated dataset
merged_data = aggregated_data.merge(venue_bands, on=['Start Date', 'Venue'], how='left')

# Save to CSV for Tableau
merged_data.to_csv('bandsatvenues.csv', index=False)



import pandas as pd

# Load the bands data
bands_data = pd.read_csv('bandsatvenues.csv')

# Aggregate bands by Venue (one list per venue)
venue_bands = bands_data.groupby('Venue')['Bands'].apply(', '.join).reset_index()

# Rename the column to 'Venue_Bands_Complete' for clarity
venue_bands.rename(columns={'Bands': 'Venue_Bands_Complete'}, inplace=True)

# Save only the Venue and the aggregated Bands data
venue_bands.to_csv('venue_bands_for_tableau.csv', index=False)

print("Successfully aggregated venue bands. Now use 'venue_bands_for_tableau.csv' in Tableau!")



import pandas as pd

# Load the bands data
bands_data = pd.read_csv('bandsatvenues.csv')

# Check the column names
print(bands_data.columns)



import pandas as pd

# Load the bands data
bands_data = pd.read_csv('bandsatvenues.csv')

# Check which 'Bands' column contains the actual band names
# We'll assume 'Bands_x' is the one with the relevant data based on the names, but double-check first
bands_data['Bands'] = bands_data['Bands_x'].fillna(bands_data['Bands_y'])

# Aggregate bands by Venue (one list per venue)
venue_bands = bands_data.groupby('Venue')['Bands'].apply(', '.join).reset_index()

# Rename the column to 'Venue_Bands_Complete' for clarity
venue_bands.rename(columns={'Bands': 'Venue_Bands_Complete'}, inplace=True)

# Save the result for Tableau
venue_bands.to_csv('venue_bands_for_tableau.csv', index=False)

print("Successfully aggregated venue bands. Now use 'venue_bands_for_tableau.csv' in Tableau!")



# Count all rows where Paid = 0
free_concert_count = (aggregated_data['Paid'] == 0).sum()
print(f"Total free concerts (Paid = 0): {free_concert_count}")

# If you need to count unique concerts where Paid = 0
# Assuming 'Concert Group' is your concert identifier column
unique_free_concerts = aggregated_data[aggregated_data['Paid'] == 0]['Concert Group'].nunique()
print(f"Unique free concerts: {unique_free_concerts}")

# To see the distribution of discount reasons for free concerts
if 'Discount' in aggregated_data.columns:
    discount_distribution = aggregated_data[aggregated_data['Paid'] == 0]['Discount'].value_counts()
    print("\nDiscount reasons for free concerts:")
    print(discount_distribution)


import pandas as pd

# Load your two main datasets
aggregated_data = pd.read_csv('final_data_aggregated.csv')
concerts_ranked = pd.read_csv('data/2024concertsranked.csv')  # Your source of truth for key fields

# Ensure Start Date is in the same format across datasets
aggregated_data['Start Date'] = pd.to_datetime(aggregated_data['Start Date'])
concerts_ranked['Start Date'] = pd.to_datetime(concerts_ranked['Start Date'])

# Apply the same venue name standardization to concerts_ranked that you applied to aggregated_data
venue_name_mapping = {
    'The Rave/Eagles Club': 'The Rave',
    'Marcus Performing Arts Center': 'Marcus Center for the Performing Arts',
    'Henry Maier Festival Park': 'Summerfest',
    'Summerfest Grounds at Henry Maier Festival Park': 'Summerfest',
    'Summerfest Grounds at Henry Maier Festival Park-M & I Bank Classic Rock Stage': 'Summerfest'
}

# Standardize venue names in concerts_ranked
concerts_ranked['Venue'] = concerts_ranked['Venue'].replace(venue_name_mapping)

# From concerts_ranked, we want the "source of truth" fields
concerts_truth = concerts_ranked[['Start Date', 'Ranking', 'Bands', 'Venue', 'Location', 'Rating', 'Paid', 'Discount']]

# From aggregated_data, we want everything except those fields that are better in concerts_ranked
columns_to_exclude = ['Ranking', 'Bands', 'Location', 'Rating', 'Paid', 'Discount']
aggregated_features = aggregated_data.drop(columns=[col for col in columns_to_exclude if col in aggregated_data.columns])

# First, check for concerts that might not match directly by venue name
print("Unique venues in concerts_ranked:", sorted(concerts_ranked['Venue'].unique()))
print("Unique venues in aggregated_data:", sorted(aggregated_data['Venue'].unique()))

# Now merge on Date and standardized Venue
combined_data = pd.merge(
    concerts_truth,
    aggregated_features,
    on=['Start Date', 'Venue'],
    how='left'
)

# Check for any rows that didn't get the aggregated data
missing_data = combined_data[combined_data['capacity'].isna()]
if not missing_data.empty:
    print(f"Warning: {len(missing_data)} concerts didn't match with aggregated data:")
    print(missing_data[['Start Date', 'Venue', 'Bands']])

# Save the combined dataset
combined_data.to_csv('combined_concert_data_final.csv', index=False)

# Preview the result
print(f"Combined data has {combined_data.shape[0]} rows and {combined_data.shape[1]} columns")
print("First few rows:")
print(combined_data.head())


import pandas as pd

# Load your datasets
concerts_ranked = pd.read_csv('data/2024concertsranked.csv')
aggregated_data = pd.read_csv('final_data_aggregated.csv')
venue_bands = pd.read_csv('venue_bands_for_tableau.csv')

# Ensure Start Date is in the same format across datasets
concerts_ranked['Start Date'] = pd.to_datetime(concerts_ranked['Start Date'])
aggregated_data['Start Date'] = pd.to_datetime(aggregated_data['Start Date'])

# Apply the same venue name standardization to concerts_ranked that you applied to aggregated_data
venue_name_mapping = {
    'The Rave/Eagles Club': 'The Rave',
    'Marcus Performing Arts Center': 'Marcus Center for the Performing Arts',
    'Henry Maier Festival Park': 'Summerfest',
    'Summerfest Grounds at Henry Maier Festival Park': 'Summerfest',
    'Summerfest Grounds at Henry Maier Festival Park-M & I Bank Classic Rock Stage': 'Summerfest'
}

concerts_ranked['Venue'] = concerts_ranked['Venue'].replace(venue_name_mapping)
venue_bands['Venue'] = venue_bands['Venue'].replace(venue_name_mapping)

# Convert slash-delimited bands to comma-delimited in concerts_ranked
concerts_ranked['Bands'] = concerts_ranked['Bands'].str.replace(' / ', ', ')

# Get fields from concerts_ranked (source of truth)
concerts_truth = concerts_ranked[['Start Date', 'Ranking', 'Bands', 'Venue', 'Location', 'Rating', 'Paid', 'Discount']]

# Get fields from aggregated_data
columns_to_exclude = ['Ranking', 'Bands', 'Location', 'Rating', 'Paid', 'Discount']
aggregated_features = aggregated_data.drop(columns=[col for col in columns_to_exclude if col in aggregated_data.columns])

# Merge datasets
combined_data = pd.merge(
    concerts_truth,
    aggregated_features,
    on=['Start Date', 'Venue'],
    how='left'
)

# Add venue_bands tooltip data
final_data = pd.merge(
    combined_data,
    venue_bands[['Venue', 'Venue_Bands_Complete']],
    on='Venue',
    how='left'
)

# Save the final consolidated dataset
final_data.to_csv('final_concert_data_with_tooltips.csv', index=False)

print(f"Final dataset has {final_data.shape[0]} rows and {final_data.shape[1]} columns")
print("First few rows:")
final_data.head()


# Re-importing the necessary libraries
import pandas as pd

# Assuming 'final_data' is already available in the environment
final_data['Start Date'] = pd.to_datetime(final_data['Start Date'])

# Filter for concerts in 2024 and count them
concerts_2024 = final_data[final_data['Start Date'].dt.year == 2024]
concerts_2024_count = concerts_2024.shape[0]
concerts_2024_count



 final_data['Discount'].value_counts()


final_data.isna().sum()


# Filter rows where any of these columns have nulls
null_rows = final_data[final_data[['Latitude', 'Longitude', 'Venue_Bands_Complete']].isna().any(axis=1)]

# Display the rows with nulls
null_rows


import pandas as pd

# Load the new dataset (long-term archive data)
concert_archives = pd.read_csv('data/ConcertArchivesExportFeb22-25.csv')

# Load the existing dataset (final data)
final_data = pd.read_csv('final_concert_data_with_tooltips.csv')

# Clean the long-term data before merging
# Rename columns in concert_archives to avoid conflicts
concert_archives = concert_archives.rename(columns={
    'Start Date': 'LongTerm_Start Date',
    'Bands': 'LongTerm_Bands',
    'Venue': 'LongTerm_Venue',
    'Location': 'LongTerm_Location'
})

# Convert 'Start Date' to datetime format (for both datasets)
concert_archives['LongTerm_Start Date'] = pd.to_datetime(concert_archives['LongTerm_Start Date'], format='%m/%d/%Y')
final_data['Start Date'] = pd.to_datetime(final_data['Start Date'], format='%Y-%m-%d')

# Convert LongTerm_Bands from " / " separators to commas
concert_archives['LongTerm_Bands'] = concert_archives['LongTerm_Bands'].str.replace(' / ', ', ')

# Perform the merge with outer join to keep all data from both datasets
combined_data = pd.merge(
    final_data,
    concert_archives,
    left_on=['Start Date', 'Venue'],
    right_on=['LongTerm_Start Date', 'LongTerm_Venue'],
    how='outer'
)

# Save the combined dataset (with both original and long-term columns)
combined_data.to_csv('combined_concert_data_with_tooltip.csv', index=False)

# Optionally, inspect the first few rows of the combined dataset
combined_data.head()
combined_data.shape


import pandas as pd

# Load the original dataset
original_data = pd.read_csv('final_concert_data_with_tooltips.csv')

# Load the merged dataset
merged_data = pd.read_csv('final_concert_data_with_tooltips.csv')

# Identify concerts in original_data not in merged_data
missing_concerts = original_data[~original_data['Start Date'].isin(merged_data['Start Date'])]

print("Missing concerts:")
print(missing_concerts)



import pandas as pd

# Load the datasets again for comparison
original_data = pd.read_csv('final_concert_data_with_tooltips.csv')
concert_archives = pd.read_csv('data/ConcertArchives_cleaned.csv')

# Check for duplicate concerts in both datasets
original_duplicates = original_data[original_data.duplicated(subset=['Start Date', 'Venue'], keep=False)]
cleaned_duplicates = concert_archives[concert_archives.duplicated(subset=['LongTerm_Start Date', 'LongTerm_Venue'], keep=False)]

# Display duplicates
print("Duplicates in original data:")
print(original_duplicates)

print("Duplicates in cleaned data:")
print(cleaned_duplicates)



# Look for concerts in the original dataset but missing in the merged dataset
missing_in_merged = original_data[~original_data[['Start Date', 'Venue']].apply(tuple, axis=1).isin(combined_data[['Start Date', 'Venue']].apply(tuple, axis=1))]

# Check how many concerts are missing and display the details
print(f"Missing concerts in merged data: {missing_in_merged.shape[0]}")
print(missing_in_merged[['Start Date', 'Venue', 'Bands']])



# Check for partial matches between the original and merged datasets
unmatched_venues = original_data[~original_data['Venue'].isin(concert_archives['LongTerm_Venue'])]
print(f"Unmatched venues in original data: {unmatched_venues[['Start Date', 'Venue']].head()}")



last_concert = original_data[original_data['Start Date'] == '2024-12-31']
print(last_concert)



print(f"Concerts in original data: {original_data.shape[0]}")
print(f"Concerts in final merged data: {combined_data.shape[0]}")



import pandas as pd

# Load the manually updated dataset
concerts_ranked = pd.read_csv('data/2024concertsranked.csv')

# Check for missing values in key columns (Start Date, Venue, Bands)
print("Missing values in key columns:")
print(concerts_ranked[['Start Date', 'Venue', 'Bands']].isna().sum())

# Check unique values in 'Venue' to see if any names are inconsistent
print("\nUnique venue names:")
print(sorted(concerts_ranked['Venue'].unique()))

# Check the overall shape of the dataset to ensure it contains the expected number of concerts
print(f"\nNumber of concerts in '2024concertsranked.csv': {concerts_ranked.shape[0]}")



# Define the mapping for venue name standardization
venue_name_mapping = {
    'The Rave/Eagles Club': 'The Rave',
    'Marcus Performing Arts Center': 'Marcus Center for the Performing Arts',
    'Henry Maier Festival Park': 'Summerfest',
    'Summerfest Grounds at Henry Maier Festival Park': 'Summerfest',
    'Summerfest Grounds at Henry Maier Festival Park-M & I Bank Classic Rock Stage': 'Summerfest'
}

# Apply the venue name standardization
concerts_ranked['Venue'] = concerts_ranked['Venue'].replace(venue_name_mapping)

# Check again for unique venue names to see the result
print("\nUpdated unique venue names after standardization:")
print(sorted(concerts_ranked['Venue'].unique()))



# Load the aggregated data
aggregated_data = pd.read_csv('final_data_aggregated.csv')

# Ensure Start Date is in the same format in both datasets
concerts_ranked['Start Date'] = pd.to_datetime(concerts_ranked['Start Date'])
aggregated_data['Start Date'] = pd.to_datetime(aggregated_data['Start Date'])

# Merge the two datasets on 'Start Date' and 'Venue'
combined_data = pd.merge(
    concerts_ranked[['Start Date', 'Ranking', 'Bands', 'Venue', 'Location', 'Rating', 'Paid', 'Discount']],  # Source of truth fields
    aggregated_data,  # Data with the extra information
    on=['Start Date', 'Venue'],
    how='left'  # Keep all concerts from concerts_ranked and add matched data from aggregated_data
)

# Check for any missing values after the merge
missing_data = combined_data[combined_data['capacity'].isna()]
if not missing_data.empty:
    print(f"Warning: {len(missing_data)} concerts didn't match with aggregated data:")
    print(missing_data[['Start Date', 'Venue', 'Bands']])

# Save the combined dataset
combined_data.to_csv('combined_concert_data_final.csv', index=False)

# Preview the combined data
print(f"Combined data has {combined_data.shape[0]} rows and {combined_data.shape[1]} columns")
print("First few rows:")
print(combined_data.head())



# Check for missing concerts in the final dataset compared to the concert archives
missing_concerts = concert_archives[~concert_archives['LongTerm_Start Date'].isin(final_data['Start Date'])]
print(missing_concerts)



import pandas as pd

# Load the datasets
combined_data = pd.read_csv('combined_concert_data_final.csv')
final_data = pd.read_csv('final_concert_data_with_tooltips.csv')

# Convert 'Start Date' to datetime to filter by year
combined_data['Start Date'] = pd.to_datetime(combined_data['Start Date'])
final_data['Start Date'] = pd.to_datetime(final_data['Start Date'])

# Filter only 2024 concerts
combined_data_2024 = combined_data[combined_data['Start Date'].dt.year == 2024]
final_data_2024 = final_data[final_data['Start Date'].dt.year == 2024]

# Find concerts in 2024 that are in combined_data but not in final_data
missing_in_final = combined_data_2024[~combined_data_2024['Start Date'].isin(final_data_2024['Start Date'])]

# Find concerts in 2024 that are in final_data but not in combined_data
missing_in_combined = final_data_2024[~final_data_2024['Start Date'].isin(combined_data_2024['Start Date'])]

# Output the results
print("Concerts in combined_data but missing in final_data:")
print(missing_in_final)

print("\nConcerts in final_data but missing in combined_data:")
print(missing_in_combined)



# View the first few rows of both datasets for 2024
print("First few rows of combined_data_2024:")
print(combined_data_2024.head())

print("\nFirst few rows of final_data_2024:")
print(final_data_2024.head())



import pandas as pd

# Load the CSV file
df = pd.read_csv('final_concert_data_with_tooltips.csv')

# Find the row containing Taylor Bennett in the Venue Bands Complete column
# Update the 'LongTerm Start Date' for that row
df.loc[df['Venue Bands Complete'].str.contains('Taylor Bennett', na=False), 'LongTerm Start Date'] = '06/21/2024'

# Save the updated DataFrame back to CSV
df.to_csv('final_concert_data_with_tooltips.csv', index=False)

print("Date updated successfully!")



import pandas as pd

# Load the CSV file
df = pd.read_csv('final_concert_data_with_tooltips.csv')

# Print the column names to confirm the exact name
print(df.columns)



import pandas as pd

# Load the CSV file
df = pd.read_csv('final_concert_data_with_tooltips.csv')

# Update the LongTerm Start Date for Taylor Bennett in Venue_Bands_Complete
df.loc[df['Venue_Bands_Complete'].str.contains('Taylor Bennett', na=False), 'LongTerm_Start Date'] = '06/21/2024'

# Print the row where Taylor Bennett is mentioned to check if the date was updated
print(df[df['Venue_Bands_Complete'].str.contains('Taylor Bennett', na=False)])

# Save the updated dataframe back to a CSV
df.to_csv('final_concert_data_with_tooltips_updated.csv', index=False)

df
