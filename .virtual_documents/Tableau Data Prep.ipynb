








import pandas as pd
import pylast
import time
from tqdm import tqdm  # For progress bars


# Load datasets
concerts = pd.read_csv("data/ConcertArchivesExportFeb22-25.csv")  # All concerts
twentyfour = pd.read_csv("data/2024concertsranked.csv")  # 2024 concerts with rankings
bestof24 = pd.read_csv("data/bestof24.csv")  # Best albums/songs of 2024
bestof23 = pd.read_csv("data/bestof23.csv")  # Best albums/songs of 2023
bestof22 = pd.read_csv("data/bestof22.csv")  # Best albums/songs of 2022
bestof21 = pd.read_csv("data/bestof21.csv")  # Best albums/songs of 2021
scrobbles = pd.read_csv("data/scrobbles.csv") #Every song I've listened to since May 2020





# Display the first few rows of each dataset
twentyfour.head()


twentyfour.shape


concerts.head()


concerts.shape


# Merge concerts and twentyfour
df_concerts = pd.merge(
    concerts,                      # Left dataset
    twentyfour,                    # Right dataset
    on=['Start Date', 'Concert Name', 'Bands', 'Venue', 'Location'],  # Key columns
    how='left'                     # Keep all rows from concerts, add matching rows from twentyfour
)

# Display the merged dataset
df_concerts.head()


df_concerts.shape


df_concerts['Location'].value_counts()


# Check for missing values in the merged dataset
print(df_concerts.isnull().sum())





# Drop End Date and Concert Name columns
df_concerts = df_concerts.drop(columns=['End Date', 'Concert Name'])
df_concerts





# Fill missing values
df_concerts['Ranking'] = df_concerts['Ranking'].fillna(0)
df_concerts['Rating'] = df_concerts['Rating'].fillna(0)
df_concerts['Paid'] = df_concerts['Paid'].fillna(1)  # Assume missing values mean paid
df_concerts['Discount'] = df_concerts['Discount'].fillna("Unknown")

# Display the updated df
df_concerts.head()


# Convert Start Date to datetime
df_concerts['Start Date'] = pd.to_datetime(df_concerts['Start Date'], format='%m/%d/%Y')

# Filter for 2024 data
df_2024 = df_concerts[df_concerts['Start Date'].dt.year == 2024]

# Display the filtered dataset
df_2024.head()





# Get unique venues
unique_venues = df_2024['Venue'].unique()

# Convert to a DataFrame
df_unique_venues = pd.DataFrame(unique_venues, columns=['Venue'])

# Display the unique venues
df_unique_venues


# Save to CSV
df_unique_venues.to_csv("data/unique_venues_2024.csv", index=False)





# Load the venue details CSV
venue_info = pd.read_csv("data/venue_info_2024.csv")

# Display the first few rows
venue_info.head()


## Merge Back with df_2024


# Merge the datasets
df_2024 = pd.merge(
    df_2024,                      # Left dataset
    venue_info,                   # Right dataset
    on='Venue',                   # Key column
    how='left'                    # Keep all rows from df_2024
)

# Display the updated dataset
df_2024.head()


# Define a mapping dictionary for venue names
venue_name_mapping = {
    'The Rave/Eagles Club': 'The Rave',
    'Marcus Performing Arts Center': 'Marcus Center for the Performing Arts',
    'Summerfest Grounds at Henry Maier Festival Park-M & I Bank Classic Rock Stage': 'Summerfest Grounds at Henry Maier Festival Park',
}

# Standardize venue names in df_2024
df_2024['Venue'] = df_2024['Venue'].replace(venue_name_mapping)

# Standardize venue names in venue_info
venue_info['Venue'] = venue_info['Venue'].replace(venue_name_mapping)



# Add a ranking column to each bestof dataset
bestof24['Rank'] = bestof24.index + 1  # Index starts at 0, so add 1
bestof23['Rank'] = bestof23.index + 1
bestof22['Rank'] = bestof22.index + 1
bestof21['Rank'] = bestof21.index + 1

# Display the updated bestof24 dataset
bestof24.head()


# Add a year column to each dataset
bestof24['Year'] = 2024
bestof23['Year'] = 2023
bestof22['Year'] = 2022
bestof21['Year'] = 2021

# Combine all bestof datasets
bestof_all = pd.concat([bestof24, bestof23, bestof22, bestof21], ignore_index=True)

# Display the combined dataset
bestof_all.head()


# Define a function to categorize albums
def categorize_album(rank):
    if rank <= 5:
        return 'Top 5'
    elif rank <= 10:
        return 'Top 10'
    elif rank <= 25:
        return 'Top 25'
    elif rank <= 50:
        return 'Top 50'
    elif rank <= 100:
        return 'Top 100'
    else:
        return 'Not in Top 100'

# Apply the function to create a new column
bestof_all['Tier'] = bestof_all['Rank'].apply(categorize_album)

# Display the updated dataset
bestof_all.head()


bestof_all.columns





# Drop unnecessary columns
bestof_all = bestof_all[['Artist Name(s)', 'Album Name', 'Release Date', 'Rank', 'Year', 'Tier']]

# Display the updated dataset
bestof_all.head()


# Define a function to extract the primary artist name
def extract_primary_artist(artist_name):
    if pd.isna(artist_name):
        return None
    # Handle special case for "Tyler, The Creator"
    if "Tyler, The Creator" in artist_name:
        return "Tyler, The Creator"
    # Split on comma and return the first artist
    return artist_name.split(',')[0].strip()

# Apply the function to create a new column
bestof_all['Primary Artist'] = bestof_all['Artist Name(s)'].apply(extract_primary_artist)

# Display the updated dataset
bestof_all.head()



bestof_all.columns





# Split the Bands column into multiple rows
df_exploded = df_2024.assign(Bands=df_2024['Bands'].str.split(' / ')).explode('Bands')

# Display the exploded dataset
df_exploded.head()


df_exploded.columns


from thefuzz import process
import re

# Ensure dates are datetime objects
scrobbles['Timestamp'] = pd.to_datetime(scrobbles['Timestamp'])
df_exploded['Start Date'] = pd.to_datetime(df_exploded['Start Date'])

# Function to clean artist names (removes featured artists, punctuation, etc.)
def clean_artist_name(artist):
    # Remove content in parentheses and after "feat." or "ft."
    artist = re.sub(r'\s*\(.*?\)|\s*feat\..*|ft\..*', '', artist, flags=re.IGNORECASE)
    return artist.strip()

# Clean scrobble artist names
scrobbles['Cleaned Artist'] = scrobbles['Artist'].apply(clean_artist_name)

# Fuzzy matching function to get closest artist match
def get_closest_artist(artist, artist_list, threshold=80):
    match, score = process.extractOne(artist, artist_list)
    return match if score >= threshold else None

# Count scrobbles for each artist before the concert date
def count_scrobbles_before_concert(row):
    concert_date = row['Start Date']
    concert_artist = row['Bands']
    
    # Get closest match for concert artist from cleaned artist list
    matched_artist = get_closest_artist(concert_artist, scrobbles['Cleaned Artist'].unique())
    
    if matched_artist:
        count = scrobbles[(scrobbles['Cleaned Artist'] == matched_artist) & 
                          (scrobbles['Timestamp'] < concert_date)].shape[0]
    else:
        count = 0
    
    return count

# Add the scrobble count to df_exploded
df_exploded['Scrobbles Before Concert'] = df_exploded.apply(count_scrobbles_before_concert, axis=1)

# Preview the updated dataframe
print(df_exploded[['Bands', 'Start Date', 'Scrobbles Before Concert']].head())



df_exploded.head()


df_exploded.columns


import pandas as pd
from datetime import datetime

# Convert release date in bestof_all to datetime
bestof_all['Release Date'] = pd.to_datetime(bestof_all['Release Date'])

# Merge the data on the artist and primary artist
merged_df = pd.merge(df_exploded, bestof_all, left_on='Bands', right_on='Primary Artist', how='left')

# Ensure 'Release Date' is not NaN before applying idxmax
most_recent_album = merged_df.dropna(subset=['Release Date']).groupby('Bands').apply(
    lambda x: x.loc[x['Release Date'].idxmax()] if not x['Release Date'].isnull().all() else pd.Series())
most_recent_album = most_recent_album[['Bands', 'Album Name', 'Release Date', 'Tier']]

# Reset index to avoid ambiguity in merge
merged_df = merged_df.reset_index(drop=True)
most_recent_album = most_recent_album.reset_index(drop=True)

# Merge this back into the original exploded dataframe
merged_df = pd.merge(df_exploded, most_recent_album, left_on='Bands', right_on='Bands', how='left')

# Calculate wait time for each concert
def calculate_wait_time(row):
    if pd.isna(row['Release Date']):
        return 'N/A'
    concert_date = pd.to_datetime(row['Start Date'])
    release_date = row['Release Date']
    delta = concert_date - release_date
    years = delta.days // 365
    months = (delta.days % 365) // 30
    days = (delta.days % 365) % 30
    if years == 0 and months == 0 and days == 0:
        return f"{years} years, {months} months, {days} days"
    return f"{years} years, {months} months, {days} days"

# Apply the wait time calculation
merged_df['Wait Time'] = merged_df.apply(calculate_wait_time, axis=1)

# Format the album and tier
merged_df['Album Info'] = merged_df.apply(
    lambda row: f"{row['Tier']} ({row['Release Date'].year}) - {row['Album Name']}" if pd.notna(row['Album Name']) else "N/A", 
    axis=1)

# Merge df_exploded with bestof_all
merged_df = pd.merge(df_exploded, bestof_all, left_on='Bands', right_on='Primary Artist', how='left')

# Drop rows with NaN in the 'Release Date' column
merged_df_cleaned = merged_df.dropna(subset=['Release Date'])

# Ensure 'Release Date' is not NaN before applying idxmax
most_recent_album = merged_df_cleaned.dropna(subset=['Release Date']).groupby('Bands').apply(
    lambda x: x.loc[x['Release Date'].idxmax()] if not x['Release Date'].isnull().all() else pd.Series())
most_recent_album = most_recent_album[['Bands', 'Album Name', 'Release Date', 'Tier']]

# Reset index again to avoid ambiguity in merge
merged_df_cleaned = merged_df_cleaned.reset_index(drop=True)
most_recent_album = most_recent_album.reset_index(drop=True)

# Merge this back into the original exploded dataframe
merged_df = pd.merge(df_exploded, most_recent_album, left_on='Bands', right_on='Bands', how='left')

# Display the final dataset
merged_df



merged_df.columns


merged_df.to_excel('data/cleaned24.xlsx', index=False)





# Load the Excel file into a DataFrame
final_data = pd.read_excel('data/cleaned24withlocalindicator.xlsx')

# View the first few rows to ensure it's loaded correctly
final_data.head()


final_data.columns


# Ensure date columns are in datetime format
final_data['Start Date'] = pd.to_datetime(final_data['Start Date'], errors='coerce')
final_data['Release Date'] = pd.to_datetime(final_data['Release Date'], errors='coerce')

# Calculate the 'Waited' column
final_data['Waited'] = final_data.apply(
    lambda row: f"{(row['Start Date'] - row['Release Date']).days} day(s)" 
    if pd.notnull(row['Release Date']) and row['Start Date'] >= row['Release Date'] else '',
    axis=1
)

# Save the updated DataFrame to a new Excel file
final_data.to_excel('final_data_with_waited_column.xlsx', index=False)

# Check the output
final_data[['Bands', 'Start Date', 'Release Date', 'Waited']]


final_data


final_data.dtypes


from geopy.geocoders import Nominatim
import pandas as pd

# Initialize the geolocator
geolocator = Nominatim(user_agent="concert_geocoder")

# Function to get the full address from the location
def get_address(location):
    try:
        # Geocode the location
        location_info = geolocator.geocode(location, timeout=10)
        if location_info:
            return location_info.address  # Full address
        else:
            return None
    except Exception as e:
        print(f"Error geocoding {location}: {e}")
        return None

# Create a 'Venue Location' column to identify unique venue-location pairs
final_data['Venue Location'] = final_data['Venue'] + ', ' + final_data['Location']

# Identify unique venue-location pairs
unique_locations = final_data['Venue Location'].drop_duplicates()

# Geocode unique locations
location_addresses = unique_locations.apply(get_address)

# Create a mapping from 'Venue Location' to 'Full Address'
location_to_address = dict(zip(unique_locations, location_addresses))

# Map the 'Full Address' back to the original dataset
final_data['Full Address'] = final_data['Venue Location'].map(location_to_address)



# Filter rows where 'Full Address' is NaN
venues_without_address = final_data[final_data['Full Address'].isna()]

# Display the venues without addresses
venues_without_address[['Venue', 'Location']]



# Dictionary mapping venues to their addresses
venue_addresses = {
    'Pool Studios': '3728 N Fratney St, Milwaukee, WI 53212',
    'Linneman\'s Riverwest Inn': '1001 E Locust St, Milwaukee, WI 53212',
    'Paradigm Coffee and Music': '1202 N 8th St, Sheboygan, WI 53081',
    'Lilliput Records': '1669 N Farwell Ave, Milwaukee, WI 53202',
    'Summerfest Grounds at Henry Maier Festival Park': '200 N Harbor Dr, Milwaukee, WI 53202',
    'Peck Pavilion, Marcus Center for the Performing Arts': '929 N Water St, Milwaukee, WI 53202'
}

# Update the 'Full Address' column based on the 'Venue' column
final_data['Full Address'] = final_data['Venue'].replace(venue_addresses)



final_data.isna().sum()


final_data[final_data['Is Local'].isna()]


# Update the 'Is Local' value for the specific concert
final_data.at[36, 'Is Local'] = 0  # 0 indicates the artist is not local


final_data.isna().sum()


final_data.to_csv('data/final_data.csv', index=False)
